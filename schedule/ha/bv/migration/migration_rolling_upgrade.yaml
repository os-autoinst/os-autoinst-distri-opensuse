---
name: migration_rolling_upgrade.yaml
description: >
  Rolling upgrade migration test on a running two nodes cluster.

  This test sets up a cluster on the previous OS version. Once the cluster
  online, the cluster stack is stopped on the first node and we proceed the
  migration to the last service pack level. The first node is rebooted and back
  online. Before doing the same operation on the second node, the cluster status
  is checked. When the two nodes are updated, we do a last check of the cluster
  stack.

  Some settings are required in the job group or test suite for this schedule to
  work.

  The other settings required in the job group are.

  CLUSTER_NAME must be defined for all jobs as a string. HA_CLUSTER_INIT must be
  defined to yes in the job that initializes the cluster
  HA_CLUSTER_JOIN must be defined for the rest of the jobs, and it must contain
  the hostname of the job that initializes the cluster (the job where HA_CLUSTER_INIT
  is defined to yes)
  HOSTNAME must be defined to different hostnames for each node.
  All jobs with the exception of the parent job must include a PARALLEL_WITH setting
  referencing the parent job.
  NICTYPE and WORKER_CLASS must be set to 'tap' in the job group directly in
  qemu based jobs. And of course, YAML_SCHEDULE must point to this file.

  Below are the optional settings.

  CTDB_TEST_ROLE can be defined as 'server' for testing CTDB. HA_CLUSTER_DRBD
  can be defined for enabling DRBD test. HA_REMOVE_NODE can be defined for
  testing a node removal. USE_DISKLESS_SBD can be defined for making a cluster
  without sbd device. USE_SYSRQ_FENCING can be defined for fencing through sysrq
  instead of 'crm node fence' command.

vars:
  # Boot qcow2 image generated by create_hdd_ha_textmode
  BOOT_HDD_IMAGE: '1'
  DESKTOP: 'textmode'
  HA_CLUSTER: '1'
  HDDMODEL: 'scsi-hd'
  # Disable qemu snapshots to avoid unexpected fences in HA tests
  QEMU_DISABLE_SNAPSHOTS: '1'
  SCC_REGISTER: 'installation'
  SCC_URL: 'https://scc.suse.com'
  TIMEOUT_SCALE: '2'
  # Defined that we are in a upgrade scenario, service pack migration to another.
  UPDATE_TYPE: 'upgrade'
  # LVM locking daemon used since SLE 15
  USE_LVMLOCKD: '1'
  USE_SUPPORT_SERVER: '1'
  VIRTIO_CONSOLE: '0'
  # Below have to be entered in the OpenQA UI because it doesn't read this YAML
  # HDD_1: SLE-%VERSION%-%ARCH%-Build%BUILD%-HA-BV.qcow2
schedule:
  - migration/version_switch_origin_system
  - boot/boot_to_desktop
  - console/suseconnect_scc
  - update/zypper_up
  - console/console_reboot
  - ha/wait_barriers
  - console/system_prepare
  - console/consoletest_setup
  - console/check_os_release
  - console/hostname
  - ha/ha_sle15_workarounds
  - ha/firewall_disable
  - ha/iscsi_client
  - ha/watchdog
  - '{{cluster_setup}}'
  - ha/check_hawk
  - ha/dlm
  - ha/clvmd_lvmlockd
  - ha/cluster_md
  - ha/vg
  - ha/filesystem
  - '{{drbd}}'
  - ha/await_upgrade_or_update
  - migration/version_switch_upgrade_target
  - ha/cluster_state_mgmt
  - migration/online_migration/zypper_migration
  - migration/online_migration/post_migration
  - ha/check_cluster_integrity
  - ha/check_hawk
  - ha/wait_others_upgraded_or_updated
  - ha/check_logs
conditional_schedule:
  cluster_setup:
    HA_CLUSTER_INIT:
      yes:
        - ha/ha_cluster_init
      no:
        - ha/ha_cluster_join
  drbd:
    HA_CLUSTER_DRBD:
      1:
        - ha/drbd_passive
        - ha/filesystem
