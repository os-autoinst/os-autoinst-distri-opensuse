<?xml version="1.0" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>lib/hacluster.pm</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link rel='stylesheet' href='./style.css' />
</head>

<body>



<ul id="index"><li><a href="./index.html"><i>&lt;= Back to file list</i></a></li>
  <li><a href="#SYNOPSIS">SYNOPSIS</a>
    <ul>
      <li><a href="#Global-Variables">Global Variables</a></li>
      <li><a href="#exec_csync">exec_csync</a></li>
      <li><a href="#add_file_in_csync">add_file_in_csync</a></li>
      <li><a href="#get_cluster_info">get_cluster_info</a></li>
      <li><a href="#get_cluster_name">get_cluster_name</a></li>
      <li><a href="#get_hostname">get_hostname</a></li>
      <li><a href="#get_node_to_join">get_node_to_join</a></li>
      <li><a href="#get_ip">get_ip</a></li>
      <li><a href="#get_my_ip">get_my_ip</a></li>
      <li><a href="#get_node_number">get_node_number</a></li>
      <li><a href="#get_node_index">get_node_index</a></li>
      <li><a href="#is_node">is_node</a></li>
      <li><a href="#add_to_known_hosts">add_to_known_hosts</a></li>
      <li><a href="#choose_node">choose_node</a></li>
      <li><a href="#save_state">save_state</a></li>
      <li><a href="#is_package_installed">is_package_installed</a></li>
      <li><a href="#check_rsc">check_rsc</a></li>
      <li><a href="#ensure_process_running">ensure_process_running</a></li>
      <li><a href="#ensure_resource_running">ensure_resource_running</a></li>
      <li><a href="#ensure_dlm_running">ensure_dlm_running</a></li>
      <li><a href="#write_tag">write_tag</a></li>
      <li><a href="#read_tag">read_tag</a></li>
      <li><a href="#block_device_real_path">block_device_real_path</a></li>
      <li><a href="#lvm_add_filter">lvm_add_filter</a></li>
      <li><a href="#lvm_remove_filter">lvm_remove_filter</a></li>
      <li><a href="#rsc_cleanup">rsc_cleanup</a></li>
      <li><a href="#ha_export_logs">ha_export_logs</a></li>
      <li><a href="#check_cluster_state">check_cluster_state</a></li>
      <li><a href="#wait_until_resources_stopped">wait_until_resources_stopped</a></li>
      <li><a href="#wait_until_resources_started">wait_until_resources_started</a></li>
      <li><a href="#wait_for_idle_cluster">wait_for_idle_cluster</a></li>
      <li><a href="#get_lun">get_lun</a></li>
      <li><a href="#check_device_available">check_device_available</a></li>
      <li><a href="#set_lvm_config">set_lvm_config</a></li>
      <li><a href="#add_lock_mgr">add_lock_mgr</a></li>
      <li><a href="#is_not_maintenance_update">is_not_maintenance_update</a></li>
      <li><a href="#activate_ntp">activate_ntp</a></li>
      <li><a href="#script_output_retry_check">script_output_retry_check</a></li>
      <li><a href="#collect_sbd_delay_parameters">collect_sbd_delay_parameters</a></li>
      <li><a href="#calculate_sbd_start_delay">calculate_sbd_start_delay</a></li>
      <li><a href="#setup_sbd_delay">setup_sbd_delay</a></li>
      <li><a href="#set_sbd_service_timeout">set_sbd_service_timeout</a></li>
      <li><a href="#check_iscsi_failure">check_iscsi_failure</a></li>
      <li><a href="#cluster_status_matches_regex">cluster_status_matches_regex</a></li>
      <li><a href="#crm_maintenance_status">crm_maintenance_status</a></li>
      <li><a href="#crm_wait_for_maintenance">crm_wait_for_maintenance</a></li>
      <li><a href="#crm_check_resource_location">crm_check_resource_location</a></li>
      <li><a href="#generate_lun_list">generate_lun_list</a></li>
    </ul>
  </li>
</ul><h1>lib/hacluster.pm</h1>

<h1 id="SYNOPSIS">SYNOPSIS</h1>

<p>Library with common methods and default values for High Availability Extension (HA or HAE) tests.</p>

<h2 id="Global-Variables">Global Variables</h2>

<ul>

<li><p><b>$default_timeout</b>: default scaled timeout for most operations with SUT</p>

</li>
<li><p><b>$join_timeout</b>: default scaled timeout for <code>ha-cluster-join</code> calls</p>

</li>
<li><p><b>$softdog_timeout</b>: default scaled timeout for the <b>softdog</b> watchdog</p>

</li>
<li><p><b>$crm_mon_cmd</b>: crm_mon (crm monitoring) command</p>

</li>
<li><p><b>$corosync_token</b>: command to filter the value of <code>runtime.config.totem.token</code> from the output of <code>corosync-cmapctl</code></p>

</li>
<li><p><b>$corosync_consensus</b>: command to filter the value of <code>runtime.config.totem.consensus</code> from the output of <code>corosync-cmapctl</code></p>

</li>
<li><p><b>$sbd_watchdog_timeout</b>: command to extract the value of <code>SBD_WATCHDOG_TIMEOUT</code> from <code>/etc/sysconfig/sbd</code></p>

</li>
<li><p><b>$sbd_delay_start</b>: command to extract the value of <code>SBD_DELAY_START</code> from <code>/etc/sysconfig/sbd</code></p>

</li>
<li><p><b>$pcmk_delay_max</b>: command to get the value of the <code>pcmd_delay_max</code> parameter from the STONITH resource in the cluster configuration.</p>

</li>
</ul>

<h2 id="exec_csync">exec_csync</h2>

<pre><code>exec_csync();</code></pre>

<p>Runs <code>csync2 -vxF</code> in the SUT, to sync files from SUT to other nodes in the cluster. Sometimes it is expected that the first call to <code>csync2 -vxF</code> fails, so this method will run the command twice.</p>

<h2 id="add_file_in_csync">add_file_in_csync</h2>

<pre><code>add_file_in_csync( value =&gt; &#39;/path/to/file&#39;, [ conf_file =&gt; &#39;/path/to/csync2.cfg&#39; ] );</code></pre>

<p>Adds <i>/path/to/file</i> to a csync2 configuration file in SUT. Path to add must be passed with the named argument <b>value</b>, while csync2 configuration file can be passed on the named argument <b>conf_file</b> (defaults to <i>/etc/csync2/csync2.cfg</i>). Returns true on success or croaks if command execution fails in SUT.</p>

<h2 id="get_cluster_info">get_cluster_info</h2>

<p>get_cluster_info();</p>

<p>Returns a hashref containing the info parsed from the CLUSTER_INFOS variable. This does not reflect the current state of the cluster but the intended steady state once the LUNs are configured and the nodes have joined.</p>

<h2 id="get_cluster_name">get_cluster_name</h2>

<pre><code>get_cluster_name();</code></pre>

<p>Returns the cluster name, as defined in the <b>CLUSTER_NAME</b> setting. Croaks if the setting is not defined, as it is a mandatory setting for HA tests.</p>

<h2 id="get_hostname">get_hostname</h2>

<pre><code>get_hostname();</code></pre>

<p>Returns the hostname, as defined in the <b>HOSTNAME</b> setting. Croaks if the setting is not defined, as it is a mandatory setting for HA tests.</p>

<h2 id="get_node_to_join">get_node_to_join</h2>

<pre><code>get_node_to_join();</code></pre>

<p>Returns the hostname of the node to join, as defined in the <b>HA_CLUSTER_JOIN</b> setting. Croaks if the setting is not defined, as this setting is mandatory for all nodes that run <code>ha-cluster-join</code>. As such, avoid scheduling tests that call this method on nodes that would run <code>ha-cluster-init</code> instead.</p>

<h2 id="get_ip">get_ip</h2>

<pre><code>get_ip( $node_hostname );</code></pre>

<p>Returns the IP address of a node given its hostname, either by calling the <code>host</code> command in SUT (which in turns would do a DNS query on tests using support server), or by searching for the host entry in SUT&#39;s <i>/etc/hosts</i>. Returns 0 on failure.</p>

<h2 id="get_my_ip">get_my_ip</h2>

<pre><code>get_my_ip();</code></pre>

<p>Returns the IP address of SUT or 0 if the address cannot be determined. Special case of <code>get_ip()</code>.</p>

<h2 id="get_node_number">get_node_number</h2>

<pre><code>get_node_number();</code></pre>

<p>Returns the number of nodes configured in the cluster.</p>

<h2 id="get_node_index">get_node_index</h2>

<pre><code>get_node_index();</code></pre>

<p>Returns the index number of the SUT. This information is taken from the node hostnames, so be sure to define proper hostnames in the tests settings, for example <b>alpha-node01</b>, <b>alpha-node02</b>, etc.</p>

<h2 id="is_node">is_node</h2>

<pre><code>is_node( $node_number );</code></pre>

<p>Checks whether SUT is the node identified by <b>$node_number</b>. Returns true or false. This information is matched against the node hostname, so be sure to define proper hostnames in the tests settings, for example <b>alpha-node01</b>, <b>alpha-node02</b>, etc.</p>

<h2 id="add_to_known_hosts">add_to_known_hosts</h2>

<pre><code>add_to_known_hosts( $host );</code></pre>

<p>Adds <b>$host</b> to the <i>.ssh/known_hosts</i> file of the current user in SUT. Croaks if any of the commands to do so fail.</p>

<h2 id="choose_node">choose_node</h2>

<pre><code>choose_node( $node_number );</code></pre>

<p>Returns the hostname of the node identified by <b>$node_number</b>. This information relies on the node hostnames, so be sure to define proper hostnames in the tests settings, for example <b>alpha-node01</b>, <b>alpha-node02</b>, etc.</p>

<h2 id="save_state">save_state</h2>

<pre><code>save_state();</code></pre>

<p>Prints the cluster configuration and cluster status in SUT, and saves the screenshot.</p>

<h2 id="is_package_installed">is_package_installed</h2>

<pre><code>is_package_installed( $package );</code></pre>

<p>Checks if <b>$package</b> is installed in SUT. Returns true or false.</p>

<h2 id="check_rsc">check_rsc</h2>

<pre><code>check_rsc( $resource );</code></pre>

<p>Checks if cluster resource <b>$resource</b> is configured in the cluster. Returns true or false.</p>

<h2 id="ensure_process_running">ensure_process_running</h2>

<pre><code>ensure_process_running( $process );</code></pre>

<p>Checks for up to <b>$default_timeout</b> seconds whether process <b>$process</b> is running in SUT. Returns 0 if process is running or croaks on timeout.</p>

<h2 id="ensure_resource_running">ensure_resource_running</h2>

<pre><code>ensure_resource_running( $resource, $regexp );</code></pre>

<p>Checks for up to <b>$default_timeout</b> seconds in the output of <code>crm resource status $resource</code> if a resource <b>$resource</b> is configured in the cluster; uses <b>$regexp</b> to check. Returns 0 on success or croaks on timeout.</p>

<h2 id="ensure_dlm_running">ensure_dlm_running</h2>

<pre><code>ensure_dlm_running();</code></pre>

<p>Checks that the <code>dlm</code> resource is running in the cluster, and that its associated process (<b>dlm_controld</b>) is running in SUT. Returns 0 if process is running or croaks on error.</p>

<h2 id="write_tag">write_tag</h2>

<pre><code>write_tag( $tag );</code></pre>

<p>Create a cluster-specific file in <i>/tmp/</i> of the SUT with <b>$tag</b> as its content. Returns 0 on success or croaks on failure.</p>

<h2 id="read_tag">read_tag</h2>

<pre><code>read_tag();</code></pre>

<p>Read the content of the cluster-specific file created in <i>/tmp/</i> with <code>write_tag()</code>. Returns the content of the file or croaks on failure.</p>

<h2 id="block_device_real_path">block_device_real_path</h2>

<pre><code>block_device_real_path( $device );</code></pre>

<p>Returns the real path of the block device specified by <b>$device</b> as shown by <code>realpath -ePL</code>, or croak on failure.</p>

<h2 id="lvm_add_filter">lvm_add_filter</h2>

<pre><code>lvm_add_filter( $type, $filter );</code></pre>

<p>Add filter <b>$filter</b> of type <b>$type</b> to <i>/etc/lvm/lvm.conf</i>.</p>

<h2 id="lvm_remove_filter">lvm_remove_filter</h2>

<pre><code>lvm_remove_filter( $filter );</code></pre>

<p>Remove filter <b>$filter</b> from <i>/etc/lvm/lvm.conf</i>.</p>

<h2 id="rsc_cleanup">rsc_cleanup</h2>

<pre><code>rsc_cleanup( $resource );</code></pre>

<p>Execute a <code>crm resource cleanup</code> on the resource identified by <b>$resource</b>.</p>

<h2 id="ha_export_logs">ha_export_logs</h2>

<pre><code>ha_export_logs();</code></pre>

<p>Upload HA-relevant logs from SUT. These include: crm configuration, cluster bootstrap log, corosync configuration, <b>crm report</b>, list of installed packages, list of iSCSI devices, <i>/etc/mdadm.conf</i>, support config and <b>y2logs</b>. If available, logs from the <b>HAWK</b> test, from <b>CTS</b> and from <b>HANA</b> are also included.</p>

<h2 id="check_cluster_state">check_cluster_state</h2>

<pre><code>check_cluster_state( [ proceed_on_failure =&gt; 1 ] );</code></pre>

<p>Checks the state of the cluster. Calls <b>$crm_mon_cmd</b> and inspects its output checking:</p>

<dl>

<dt id="The-current-state-of-the-cluster">The current state of the cluster.</dt>
<dd>

</dd>
<dt id="Inactive-resources">Inactive resources.</dt>
<dd>

</dd>
<dt id="partition-with-quorum"><span style="white-space: nowrap;">partition with quorum</span></dt>
<dd>

</dd>
</dl>

<p>Checks that the reported number of nodes in the output of <code>crm node list</code> and <b>$crm_mon_cmd</b> is the same.</p>

<p>And runs <code>crm_verify -LV</code>.</p>

<p>With the named argument <b>proceed_on_failure</b> set to 1, the function will use <b>script_run()</b> and attempt to run all commands in SUT without checking for errors. Without it, the method uses <b>assert_script_run()</b> and will croak on failure.</p>

<h2 id="wait_until_resources_stopped">wait_until_resources_stopped</h2>

<pre><code>wait_until_resources_stopped( [ timeout =&gt; $timeout, minchecks =&gt; $tries ] );</code></pre>

<p>Wait for resources to be stopped. Runs <b>$crm_mon_cmd</b> until there are no resources in <b>stopping</b> state or up to <b>$timeout</b> seconds. Timeout must be specified by the named argument <b>timeout</b> (defaults to 120 seconds). This timeout is scaled by the factor specified in the <b>TIMEOUT_SCALE</b> setting. The named argument <b>minchecks</b> (defaults to 3, can be disabled with 0) provides a minimum number of times to check independently of the return status; this helps avoid race conditions where the method checks before the HA stack starts to stop the resources. Croaks on timeout.</p>

<h2 id="wait_until_resources_started">wait_until_resources_started</h2>

<pre><code>wait_until_resources_started( [ timeout =&gt; $timeout ] );</code></pre>

<p>Wait for resources to be started. Runs <code>crm cluster wait_for_startup</code> in SUT as well as other verifications on newer versions of SLES (12-SP3+), for up to <b>$timeout</b> seconds for each command. Timeout must be specified by the named argument <b>timeout</b> (defaults to 120 seconds). This timeout is scaled by the factor specified in the <b>TIMEOUT_SCALE</b> setting. Croaks on timeout.</p>

<h2 id="wait_for_idle_cluster">wait_for_idle_cluster</h2>

<pre><code>wait_for_idle_cluster( [ timeout =&gt; $timeout ] );</code></pre>

<p>Use <code>cs_wait_for_idle</code> to wait until the cluster is idle before continuing the tests. Supply a timeout with the named argument <b>timeout</b> (defaults to 120 seconds). This timeout is scaled by the factor specified in the <b>TIMEOUT_SCALE</b> setting. Croaks on timeout.</p>

<h2 id="get_lun">get_lun</h2>

<pre><code>get_lun( [ use_once =&gt; $bool ] );</code></pre>

<p>Returns a LUN from the LUN list file stored in the support server or in the support NFS share in scenarios without support server. If the named argument <b>use_once</b> is passed and set to true (defaults to true), the returned LUN will be removed from the file, so it will not be selected again. Croaks on failure.</p>

<h2 id="check_device_available">check_device_available</h2>

<pre><code>check_device_available( $device, [ $timeout ] );</code></pre>

<p>Checks for the presence of a device in the SUT for up to a defined timeout (defaults to 20 seconds). Returns 0 on success, or croaks on failure.</p>

<h2 id="set_lvm_config">set_lvm_config</h2>

<pre><code>set_lvm_config( $lvm_config_file, [ use_lvmetad =&gt; $val1, locking_type =&gt; $val2, use_lvmlockd =&gt; $val3, ... ] );</code></pre>

<p>Configures the LVM parameters/values pairs passed as a HASH into the LVM configuration file specified by the first argument <b>$lvm_config_file</b>. These LVM parameters are usually <b>use_lvmetad</b>, <b>locking_type</b> and <b>use_lvmlockd</b> but any other existing parameter from the LVM configuration file is also valid. Parameters that do not exist in the LVM configuration file in SUT will be ignored. Returns 0 on success or croaks on failure.</p>

<h2 id="add_lock_mgr">add_lock_mgr</h2>

<pre><code>add_lock_mgr( $lock_manager );</code></pre>

<p>Configures a <b>$lock_manager</b> resource in the cluster configuration on SUT. <b>$lock_mgr</b> usually is either <b>clvmd</b> or <b>lvmlockd</b>, but any other cluster primitive could work as well.</p>

<h2 id="is_not_maintenance_update">is_not_maintenance_update</h2>

<pre><code>is_not_maintenance_update( $package );</code></pre>

<p>Checks if the package specified in <b>$package</b> is not targeted by a maintenance update. Returns true if the package is not targeted, i.e., <b>MAINTENANCE</b> setting is active and package name does not appear in the <b>BUILD</b> setting nor is it in the list of packages in the related <b>INCIDENT_ID</b>. Returns false in all other cases. Besides the package <b>$package</b>, it also checks for <b>kernel</b> in the <b>BUILD</b> setting and the list of packages, as the tests should always run with updates to the kernel.</p>

<h2 id="activate_ntp">activate_ntp</h2>

<pre><code>activate_ntp();</code></pre>

<p>Enables NTP service in SUT.</p>

<h2 id="script_output_retry_check">script_output_retry_check</h2>

<pre><code>script_output_retry_check(cmd=&gt;$cmd, regex_string=&gt;$regex_sring, [retry=&gt;$retry, sleep=&gt;$sleep, ignore_failure=&gt;$ignore_failure]);</code></pre>

<p>Executes command via <code>script_output</code> subroutine and makes a sanity check against a regular expression. Command output is returned after success, otherwise the command is retried a defined number of times. Test dies after last unsuccessfull retry.</p>

<p><b>$cmd</b> command being executed.</p>

<p><b>$regex_string</b> regular expression to check output against.</p>

<p><b>$retry</b> number of retries. Defaults to <code>5</code>.</p>

<p><b>$sleep</b> sleep time between retries. Defaults to <code>10s</code>.</p>

<p><b>$ignore_failure</b> do not kill the test upon failure.</p>

<pre><code>Example: script_output_retry_check(cmd=&gt;&#39;hostname&#39;, regex_string=&gt;&#39;^node01$&#39;, retry=&gt;&#39;100&#39;, sleep=&gt;&#39;60&#39;, ignore_failure=&gt;&#39;1&#39;);</code></pre>

<h2 id="collect_sbd_delay_parameters">collect_sbd_delay_parameters</h2>

<pre><code>collect_sbd_delay_parameters();</code></pre>

<p>Collects a series of SBD parameters from the SUT and returns them in a HASH format. Commands are collected from <code>/etc/sysconfig/sbd</code> or by filtering the output of <code>corosync-cmapctl</code>. Due to possible race conditions, all these parameters are collected using the helper function <code>script_output_retry_check</code> also defined in this library.</p>

<h2 id="calculate_sbd_start_delay">calculate_sbd_start_delay</h2>

<pre><code>calculate_sbd_start_delay(\%sbd_parameters);</code></pre>

<p>Calculates start time delay after node is fenced. This delay time is used as a wait time after a node fence to prevent cluster failures in cases where the fenced node restarts too quickly. Delay time is used either if specified in sbd config variable <b>SBD_DELAY_START</b> or calculated by the formula:</p>

<p>corosync token timeout + consensus timeout + pcmk_delay_max + msgwait</p>

<p>Variables <b>corosync_token</b> and <b>corosync_consensus</b> are converted to seconds. For diskless SBD pcmk_delay_max is set to static 30s.</p>

<pre><code>%sbd_parameters = {
    &#39;corosync_token&#39; =&gt; &lt;runtime.config.totem.token&gt;,
    &#39;corosync_consensus&#39; =&gt; &lt;runtime.config.totem.consensus&gt;,
    &#39;sbd_watchdog_timeout&#39; =&gt; &lt;SBD_WATCHDOG_TIMEOUT&gt;,
    &#39;sbd_delay_start&#39; =&gt; &lt;SBD_DELAY_START&gt;,
    &#39;pcmk_delay_max&#39; =&gt; &lt;pcmk_delay_max&gt;
}</code></pre>

<p>If <code>%sbd_parameters</code> argument is omitted, then function will try to obtain the values from the configuration files. See <code>collect_sbd_delay_parameters</code></p>

<h2 id="setup_sbd_delay">setup_sbd_delay</h2>

<pre><code>setup_sbd_delay()</code></pre>

<p>This function configures in the SUT the <b>SBD_DELAY_START</b> parameter in <code>/etc/sysconfig/sbd</code> to whatever value is supplied in the setting <b>HA_SBD_START_DELAY</b>, and then call <code>calculate_sbd_start_delay</code> and <code>set_sbd_service_timeout</code> to set the service timeout for the SBD service in the SUT. It returns the calculated delay. Will croak if any of the commands sent to the SUT fail.</p>

<h2 id="set_sbd_service_timeout">set_sbd_service_timeout</h2>

<pre><code>set_sbd_service_timeout($service_timeout)</code></pre>

<p>Set the service timeout for the SBD service in the SUT to the number of seconds passed as argument.</p>

<p>This is accomplished by configuring a systemd override file for the SBD service.</p>

<p>If the override file exists, the function will edit it and replace the timeout there, otherwise it creates the file from scratch.</p>

<h2 id="check_iscsi_failure">check_iscsi_failure</h2>

<pre><code>check_iscsi_failure();</code></pre>

<p>Workaround for bsc#1129385, checks system log for iSCSI connection failures, if necessary restarts iscsi and pacemaker service</p>

<h2 id="cluster_status_matches_regex">cluster_status_matches_regex</h2>

<p>Check crm status output against a hardcode regular expression in order to check the cluster health</p>

<dl>

<dt id="SHOW_CLUSTER_STATUS---Output-from-crm-status-command"><b>SHOW_CLUSTER_STATUS</b> - Output from &#39;crm status&#39; command</dt>
<dd>

</dd>
</dl>

<h2 id="crm_maintenance_status">crm_maintenance_status</h2>

<pre><code>crm_maintenance_status();</code></pre>

<p>Check maintenance mode status. Returns true (maintenance active) or false (maintenance inactive). Croaks if unknown status is received.</p>

<h2 id="crm_wait_for_maintenance">crm_wait_for_maintenance</h2>

<pre><code>crm_wait_for_maintenance(target_state=&gt;$target_state, [loop_sleep=&gt;$loop_sleep, timeout=&gt;$timeout]);</code></pre>

<p>Wait for maintenance to be turned on or off. Croaks on timeout.</p>

<ul>

<p><b>target_state</b> Target state of the maintenance mode (true/false)</p>

<p><b>loop_sleep</b> Override default sleep value between checks</p>

<p><b>timeout</b> Override default timeout value</p>

</ul>

<h2 id="crm_check_resource_location">crm_check_resource_location</h2>

<pre><code>crm_check_resource_location(resource=&gt;$resource, [wait_for_target=&gt;$wait_for_target, timeout=&gt;$timeout]);</code></pre>

<p>Checks current resource location, returns hostname of the node. Can be used to wait for desired state Eg: after failover. Croaks upon timeout.</p>

<ul>

<p><b>wait_for_target</b> Target location of the resource specified - physical hostname</p>

<p><b>resource</b> Resource to check</p>

<p><b>timeout</b> Override default timeout value</p>

</ul>

<h2 id="generate_lun_list">generate_lun_list</h2>

<pre><code>generate_lun_list()</code></pre>

<p>This generates the information that nodes need to use iSCSI. This is stored in /tmp/$cluster_name-lun.list where nodes can get it using scp.</p>


</body>

</html>


