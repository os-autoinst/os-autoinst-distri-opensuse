<?xml version="1.0" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>lib/hpcbase.pm</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link rel='stylesheet' href='./style.css' />
</head>

<body>



<ul id="index"><li><a href="./index.html"><i>&lt;= Back to file list</i></a></li>
  <li>
    <ul>
      <li><a href="#enable_and_start">enable_and_start</a></li>
      <li><a href="#master_node_names">master_node_names</a></li>
      <li><a href="#slave_node_names">slave_node_names</a></li>
      <li><a href="#cluster_names">cluster_names</a></li>
      <li><a href="#distribute_munge_key">distribute_munge_key</a></li>
      <li><a href="#distribute_slurm_conf">distribute_slurm_conf</a></li>
      <li><a href="#generate_and_distribute_ssh">generate_and_distribute_ssh</a></li>
      <li><a href="#check_nodes_availability">check_nodes_availability</a></li>
      <li><a href="#mount_nfs">mount_nfs</a></li>
      <li><a href="#get_master_ip">get_master_ip</a></li>
      <li><a href="#get_slave_ip">get_slave_ip</a></li>
      <li><a href="#prepare_user_and_group">prepare_user_and_group</a></li>
      <li><a href="#prepare_spack_env">prepare_spack_env</a></li>
      <li><a href="#uninstall_spack_module">uninstall_spack_module</a></li>
      <li><a href="#get_compute_nodes_deps">get_compute_nodes_deps</a></li>
      <li><a href="#CAVEATS">CAVEATS</a></li>
      <li><a href="#setup_nfs_server">setup_nfs_server</a></li>
      <li><a href="#mount_nfs_exports">mount_nfs_exports</a></li>
    </ul>
  </li>
</ul><h1>lib/hpcbase.pm</h1>

<h2 id="enable_and_start">enable_and_start</h2>

<p>Enables and starts given systemd service</p>

<h2 id="master_node_names">master_node_names</h2>

<p>Prepare master node names, so those names could be reused, for instance in config preparation, munge key distribution, etc. The naming follows general pattern of master-slave</p>

<h2 id="slave_node_names">slave_node_names</h2>

<p>Prepare compute node names, so those names could be reused, for instance in config preparation, munge key distribution, etc. The naming follows general pattern of master-slave</p>

<h2 id="cluster_names">cluster_names</h2>

<p>Prepare all node names, so those names could be reused</p>

<h2 id="distribute_munge_key">distribute_munge_key</h2>

<p>Distributes munge keys across all compute nodes of the cluster. This should usually be called from the master node. If a replica master node is expected, key should be also be copied in it too.</p>

<h2 id="distribute_slurm_conf">distribute_slurm_conf</h2>

<p>Distributes slurm config across all compute nodes of the cluster This should usually be called from the master node. If a replica master node is expected, config file should be also be copied in it too.</p>

<h2 id="generate_and_distribute_ssh">generate_and_distribute_ssh</h2>

<pre><code>     generate_and_distribute_ssh($user)</code></pre>

<p>Generates and distributes ssh keys across compute nodes. <code>user</code> by default is set to <b>root</b> user unless another value is passed to the parameters. <code>user</code> is used to determine the user on the remote machine where the ssh_id will be copied. This should usually be called from the master node. If a replica master node is expected, the ssh keys should be also be distributed in it too.</p>

<h2 id="check_nodes_availability">check_nodes_availability</h2>

<p>Checks if all listed HPC cluster nodes are available (ping)</p>

<h2 id="mount_nfs">mount_nfs</h2>

<p>Ensure correct dir is created, and correct NFS dir is mounted on SUT</p>

<h2 id="get_master_ip">get_master_ip</h2>

<p>Check the IP of the master node</p>

<h2 id="get_slave_ip">get_slave_ip</h2>

<p>Check the IP of the slave node</p>

<h2 id="prepare_user_and_group">prepare_user_and_group</h2>

<p>Creating slurm user and group with some pre-defined ID</p>

<h2 id="prepare_spack_env">prepare_spack_env</h2>

<pre><code>  prepare_spack_env($mpi)</code></pre>

<p>After install spack and HPC <code>mpi</code> required packages, prepares env variables. The HPC packages (*-gnu-hpc) use an installation path that is separate from the rest and can be exported via a network file system.</p>

<p>After <code>prepare_spack_env</code> run, <code>spack</code> should be ready to build entire tool stack, downloading and installing all bits required for whatever package or compiler.</p>

<h2 id="uninstall_spack_module">uninstall_spack_module</h2>

<pre><code>  uninstall_spack_module($module)</code></pre>

<p>Unload and uninstall <code>module</code> from spack stack</p>

<h2 id="get_compute_nodes_deps">get_compute_nodes_deps</h2>

<pre><code>  get_compute_nodes_deps($mpi)</code></pre>

<p>This function is used to select dependencies packages which are required to be installed on HPC compute nodes in order to run code against particular <code>mpi</code> implementation. <code>get_compute_nodes_deps</code> returns an array of packages</p>

<h2 id="CAVEATS">CAVEATS</h2>

<p>Obsolete function. Not in use since sle15sp5 Used to install dependencies of the HPC modules when the binaries were shared through NFS. Changes in openmpi breaks this on SLE15SP5. Need to get updated to be functional again. As for now can be used to find those dependencies prior to that version.</p>

<h2 id="setup_nfs_server">setup_nfs_server</h2>

<p>Prepare a nfs server on the so called management node of the HPC setup. The management node in a minimal setup should provide the directories of *-gnu-hpc installed libraries and the directory with the binaries.</p>

<p><code>exports</code> takes a hash reference with the paths which NFS should make available to the compute nodes in order to run MPI software.</p>

<h2 id="mount_nfs_exports">mount_nfs_exports</h2>

<p>Make the HPC libraries and the location of the binaries available to the so called compute nodes, from the management one. <code>exports</code> takes a hash reference with the paths which the management node share in order to run the MPI binaries</p>


</body>

</html>


